# ğŸ§ª CHemiAI Repository Architecture Diagram

> **A PyTorch-based deep learning system for mesh analysis with texture-geometry integration and automatic mesh slicing**

---

## ğŸ“ Complete Directory Structure

```
CHemiAI/
â”‚
â”œâ”€â”€ ğŸ“‹ Project Configuration
â”‚   â”œâ”€â”€ requirements.txt                          # Python dependencies
â”‚   â”œâ”€â”€ config_inference_auto_slice_example.yaml  # Auto-slice inference config
â”‚   â”œâ”€â”€ config_inference_with_slicing.yaml        # Standard slicing config
â”‚   â””â”€â”€ AUTO_SLICE_README.md                      # Auto-slicing documentation
â”‚
â”œâ”€â”€ ğŸš€ Entry Points
â”‚   â”œâ”€â”€ train.py                                  # Main training script
â”‚   â”‚   â”œâ”€ EMA (Exponential Moving Average)
â”‚   â”‚   â”œâ”€ Mixed Precision Training (AMP)
â”‚   â”‚   â”œâ”€ Checkpoint Management
â”‚   â”‚   â””â”€ Metrics: IoU, F1, Accuracy
â”‚   â”‚
â”‚   â””â”€â”€ Infer.py                                  # Inference & auto-slicing
â”‚       â”œâ”€ Auto-slicing (grid/adaptive modes)
â”‚       â”œâ”€ Batch prediction
â”‚       â””â”€ Result aggregation
â”‚
â”œâ”€â”€ ğŸ§  Core Models
â”‚   â”œâ”€â”€ model_G_2.py                              # Transformer-based nomeformer
â”‚   â”‚   â”œâ”€ MultiHeadAttention                     # Self-attention with positional encoding
â”‚   â”‚   â”œâ”€ TransformerBlock                       # Attention + FFN with residual
â”‚   â”‚   â”œâ”€ VertexEmbedding                        # Embed vertices (XYZ + normals)
â”‚   â”‚   â”œâ”€ FaceEmbedding                          # Aggregate vertices to faces
â”‚   â”‚   â””â”€ nomeformer                             # Main transformer architecture
â”‚   â”‚
â”‚   â””â”€â”€ integrated_texture_geometry_model.py      # Texture + Geometry fusion
â”‚       â”œâ”€ TexturePatchEmbedding                  # Process RGB pixel sequences
â”‚       â”œâ”€ IntegratedTextureGeometryModel         # Multi-modal fusion
â”‚       â”‚   â”œâ”€ Geometry Branch (vertices/faces)
â”‚       â”‚   â”œâ”€ Texture Branch (RGB pixels)
â”‚       â”‚   â””â”€ Cross-Modal Attention
â”‚       â””â”€ IntegratedDownstreamClassifier         # Classification head
â”‚
â”œâ”€â”€ ğŸ“Š Data Processing
â”‚   â”œâ”€â”€ mesh_dataset_2.py                         # Geometry dataset
â”‚   â”‚   â”œâ”€ MeshDataset                            # Load .ply meshes
â”‚   â”‚   â”œâ”€ MeshAugmentation                       # Rotation, scaling, jittering
â”‚   â”‚   â”œâ”€ compute_cluster_centroids              # Spatial clustering
â”‚   â”‚   â”œâ”€ reorder_clusters_by_proximity          # MST-based ordering
â”‚   â”‚   â””â”€ custom_collate_fn                      # Batch collation
â”‚   â”‚
â”‚   â””â”€â”€ mesh_texture_dataset.py                   # Texture dataset
â”‚       â”œâ”€ MeshTextureDataset                     # Load meshes + texture pixels
â”‚       â”œâ”€ texture_custom_collate_fn              # Texture batch collation
â”‚       â””â”€ Inherits augmentation from mesh_dataset_2
â”‚
â”œâ”€â”€ ğŸ› ï¸ Utilities & Tools
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ downst.py                             # Downstream classifier
â”‚       â”‚   â””â”€ DownstreamClassifier               # MLP classifier head
â”‚       â”‚
â”‚       â”œâ”€â”€ helper.py                             # Training helpers
â”‚       â”‚   â””â”€ init_opt()                         # Optimizer initialization
â”‚       â”‚
â”‚       â”œâ”€â”€ check_point.py                        # Model persistence
â”‚       â”‚   â”œâ”€ save_checkpoint()                  # Save model state
â”‚       â”‚   â””â”€ load_checkpoint()                  # Load model state
â”‚       â”‚
â”‚       â””â”€â”€ auto_mesh_slicer.py                   # Mesh slicing system
â”‚           â”œâ”€ parse_ply_file()                   # Read PLY format
â”‚           â”œâ”€ generate_automatic_bounding_boxes() # Grid-based slicing
â”‚           â”œâ”€ generate_adaptive_bounding_boxes() # Density-based slicing
â”‚           â”œâ”€ slice_mesh_with_bounding_boxes()   # Extract mesh slices
â”‚           â””â”€ write_ply_file()                   # Write PLY format
â”‚
â”œâ”€â”€ ğŸ”§ Source Components
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ train.py                              # Training loop utilities
â”‚       â”œâ”€â”€ transforms.py                         # Data transformations
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ distributed.py                    # Multi-GPU training
â”‚           â”œâ”€â”€ logging.py                        # Training logs
â”‚           â”œâ”€â”€ schedulers.py                     # LR schedulers
â”‚           â””â”€â”€ tensors.py                        # Tensor utilities
â”‚
â”œâ”€â”€ âš™ï¸ Loss Functions
â”‚   â””â”€â”€ loss.py
â”‚       â””â”€ MaskedCrossEntropyLoss                 # Handles variable-length sequences
â”‚
â””â”€â”€ ğŸ“¦ Cache & Build
    â””â”€â”€ __pycache__/                              # Python bytecode cache
```

---

## ğŸ”— Module Dependencies & Relationships

```mermaid
graph TB
    subgraph "ğŸš€ Entry Points"
        TRAIN[<b>train.py</b><br/>Training Pipeline<br/>â”â”â”â”â”â”â”â”â”â”<br/>â€¢ EMA Support<br/>â€¢ Mixed Precision<br/>â€¢ Checkpointing]
        INFER[<b>Infer.py</b><br/>Inference Pipeline<br/>â”â”â”â”â”â”â”â”â”â”<br/>â€¢ Auto-Slicing<br/>â€¢ Batch Processing<br/>â€¢ Metrics]
    end

    subgraph "ğŸ“Š Data Loading"
        MESH_DS[<b>mesh_dataset_2.py</b><br/>â”â”â”â”â”â”â”â”â”â”<br/>MeshDataset<br/>MeshAugmentation<br/>Clustering]
        TEXTURE_DS[<b>mesh_texture_dataset.py</b><br/>â”â”â”â”â”â”â”â”â”â”<br/>MeshTextureDataset<br/>Texture Processing]
    end

    subgraph "ğŸ§  Model Architecture"
        MODEL[<b>model_G_2.py</b><br/>â”â”â”â”â”â”â”â”â”â”<br/>nomeformer<br/>MultiHeadAttention<br/>TransformerBlock]
        INTEGRATED[<b>integrated_texture<br/>_geometry_model.py</b><br/>â”â”â”â”â”â”â”â”â”â”<br/>Texture-Geometry Fusion<br/>Cross-Modal Attention]
        DOWNST[<b>tools/downst.py</b><br/>â”â”â”â”â”â”â”â”â”â”<br/>DownstreamClassifier]
    end

    subgraph "âš™ï¸ Training Components"
        LOSS[<b>loss.py</b><br/>â”â”â”â”â”â”â”â”â”â”<br/>MaskedCrossEntropyLoss]
        HELPER[<b>tools/helper.py</b><br/>â”â”â”â”â”â”â”â”â”â”<br/>init_opt]
        CHECKPOINT[<b>tools/check_point.py</b><br/>â”â”â”â”â”â”â”â”â”â”<br/>save/load checkpoint]
    end

    subgraph "ğŸ”ª Mesh Slicing"
        SLICER[<b>tools/auto_mesh_slicer.py</b><br/>â”â”â”â”â”â”â”â”â”â”<br/>Bounding Box Generation<br/>Grid & Adaptive Modes]
    end

    subgraph "ğŸ”§ Utilities"
        UTILS[<b>src/utils/</b><br/>â”â”â”â”â”â”â”â”â”â”<br/>distributed.py<br/>logging.py<br/>schedulers.py<br/>tensors.py]
    end

    %% Training Flow
    TRAIN -->|loads| MESH_DS
    TRAIN -->|loads| TEXTURE_DS
    TRAIN -->|initializes| MODEL
    TRAIN -->|initializes| INTEGRATED
    TRAIN -->|uses| DOWNST
    TRAIN -->|computes loss| LOSS
    TRAIN -->|optimizer| HELPER
    TRAIN -->|saves/loads| CHECKPOINT
    TRAIN -->|utilities| UTILS

    %% Inference Flow
    INFER -->|preprocesses| SLICER
    INFER -->|loads| MESH_DS
    INFER -->|loads| TEXTURE_DS
    INFER -->|runs| MODEL
    INFER -->|runs| INTEGRATED
    INFER -->|uses| DOWNST
    INFER -->|loads from| CHECKPOINT

    %% Model Dependencies
    INTEGRATED -->|extends| MODEL
    DOWNST -->|uses| MODEL
    TEXTURE_DS -->|inherits from| MESH_DS

    style TRAIN fill:#90EE90,stroke:#2d5016,stroke-width:3px,color:#000
    style INFER fill:#87CEEB,stroke:#16335d,stroke-width:3px,color:#000
    style MODEL fill:#FFB6C1,stroke:#5d1625,stroke-width:3px,color:#000
    style INTEGRATED fill:#FFB6C1,stroke:#5d1625,stroke-width:3px,color:#000
    style SLICER fill:#DDA0DD,stroke:#4a1654,stroke-width:3px,color:#000
    style LOSS fill:#F0E68C,stroke:#5d5416,stroke-width:3px,color:#000
```

---

## ğŸ”„ Complete Data Flow Pipeline

```mermaid
flowchart TD
    subgraph INPUT["ğŸ“¥ INPUT DATA"]
        MESH[ğŸ—‚ï¸ Mesh Files<br/>.ply format<br/>vertices + faces + labels]
        TEXTURE[ğŸ¨ Texture Data<br/>RGB pixel sequences<br/>mapped to faces]
        CONFIG[âš™ï¸ Config YAML<br/>model params<br/>training settings<br/>auto-slice config]
    end

    subgraph PREPROCESS["ğŸ”§ PREPROCESSING"]
        SLICE{Auto-Slice<br/>Enabled?}
        GRID[Grid Mode<br/>Fixed divisions<br/>e.g., 3Ã—3Ã—1]
        ADAPTIVE[Adaptive Mode<br/>Density-based<br/>target faces/box]
        AUG[ğŸ”„ Augmentation<br/>â€¢ Rotation<br/>â€¢ Scaling<br/>â€¢ Jittering<br/>â€¢ Noise]
        CLUSTER[ğŸ“ Clustering<br/>KMeans spatial<br/>MST ordering]
    end

    subgraph MODEL_ARCH["ğŸ§  MODEL ARCHITECTURE"]
        VERT_EMB[Vertex Embedding<br/>XYZ + Normals â†’ embed_dim]
        PIXEL_PROJ[Pixel Projection<br/>RGB â†’ embed_dim]
        
        FACE_ATTN[Face-Level Attention<br/>Aggregate vertices per face]
        PIXEL_ATTN[Pixel-Level Attention<br/>Summarize pixels to CLS token]
        
        CLUSTER_ATTN[Cluster-Level Attention<br/>Process faces within clusters]
        
        GLOBAL_ATTN[Global Attention<br/>Cross-cluster relationships]
        
        FUSION[ğŸ”— Cross-Modal Fusion<br/>Geometry â†” Texture<br/>Cross-Attention]
        
        CLASSIFIER[Downstream Classifier<br/>MLP layers<br/>Face-wise predictions]
    end

    subgraph OUTPUT["ğŸ“¤ OUTPUT"]
        PRED[ğŸ¯ Predictions<br/>Face labels<br/>per-class probabilities]
        METRICS[ğŸ“Š Metrics<br/>â€¢ IoU<br/>â€¢ F1 Score<br/>â€¢ Accuracy]
        VIS[ğŸ–¼ï¸ Visualization<br/>Labeled meshes<br/>saved as .ply]
        CKPT[ğŸ’¾ Checkpoints<br/>.pth files<br/>model state]
    end

    %% Flow connections
    MESH --> SLICE
    CONFIG --> SLICE
    
    SLICE -->|Yes| GRID
    SLICE -->|Yes| ADAPTIVE
    SLICE -->|No| AUG
    GRID --> AUG
    ADAPTIVE --> AUG
    
    AUG --> CLUSTER
    TEXTURE --> CLUSTER
    
    CLUSTER --> VERT_EMB
    CLUSTER --> PIXEL_PROJ
    
    VERT_EMB --> FACE_ATTN
    PIXEL_PROJ --> PIXEL_ATTN
    
    FACE_ATTN --> CLUSTER_ATTN
    PIXEL_ATTN --> CLUSTER_ATTN
    
    CLUSTER_ATTN --> GLOBAL_ATTN
    GLOBAL_ATTN --> FUSION
    
    FUSION --> CLASSIFIER
    CLASSIFIER --> PRED
    
    PRED --> METRICS
    PRED --> VIS
    GLOBAL_ATTN --> CKPT
    FUSION --> CKPT

    style INPUT fill:#E6F3FF,stroke:#0066CC,stroke-width:2px
    style PREPROCESS fill:#FFF9E6,stroke:#CC9900,stroke-width:2px
    style MODEL_ARCH fill:#FFE6F0,stroke:#CC0066,stroke-width:2px
    style OUTPUT fill:#E6FFE6,stroke:#00CC66,stroke-width:2px
```

---

## ğŸ—ï¸ Model Architecture Deep Dive

### 1ï¸âƒ£ Geometry-Only Model (nomeformer)

```
Input: Mesh with N faces
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vertex Embedding Layer             â”‚
â”‚  Input: [B, C, F, V, 6]             â”‚ B=batch, C=clusters,
â”‚  (XYZ + Normals)                    â”‚ F=faces, V=vertices
â”‚  Output: [B, C, F, V, embed_dim]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Face-Level Transformer             â”‚
â”‚  â€¢ Multi-Head Self-Attention        â”‚
â”‚  â€¢ Aggregate V vertices â†’ 1 face    â”‚
â”‚  â€¢ Output: [B, C, F, embed_dim]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cluster-Level Transformer          â”‚
â”‚  â€¢ Process F faces within cluster   â”‚
â”‚  â€¢ Local spatial relationships      â”‚
â”‚  â€¢ Output: [B, C, F, embed_dim]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Global Transformer                 â”‚
â”‚  â€¢ Cross-cluster attention          â”‚
â”‚  â€¢ Learn global structure           â”‚
â”‚  â€¢ Output: [B, C, F, embed_dim]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Downstream Classifier              â”‚
â”‚  â€¢ MLP layers                       â”‚
â”‚  â€¢ Output: [B, C, F, num_classes]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2ï¸âƒ£ Integrated Texture-Geometry Model

```
                    Input Mesh
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                               â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ GEOMETRY BRANCH   â”ƒ         â”ƒ  TEXTURE BRANCH   â”ƒ
â”ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”ƒ         â”ƒ  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”ƒ
â”ƒ Vertices (XYZ)    â”ƒ         â”ƒ  RGB Pixels       â”ƒ
â”ƒ Normals           â”ƒ         â”ƒ  per Face         â”ƒ
â”ƒ [B,C,F,V,6]       â”ƒ         â”ƒ  [B,C,F,P,3]      â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›         â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
        â†“                               â†“
    Vertex                          Pixel
    Embedding                       Projection
    [embed_dim]                     [embed_dim]
        â†“                               â†“
    Face-Level                      Local
    Attention                       Attention
    (agg vertices)                  (CLS token)
        â†“                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     [B,C,F,embed_dim]   [B,C,F,embed_dim] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
        â”ƒ  CROSS-MODAL FUSION          â”ƒ
        â”ƒ  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”ƒ
        â”ƒ  â€¢ Geometry-to-Texture Attn  â”ƒ
        â”ƒ  â€¢ Texture-to-Geometry Attn  â”ƒ
        â”ƒ  â€¢ Feature Concatenation     â”ƒ
        â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                        â†“
                [B,C,F,2*embed_dim]
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Projection to embed_dim  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
                [B,C,F,embed_dim]
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Cluster Transformer      â”‚
        â”‚  Global Transformer       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Integrated Classifier    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
                [B,C,F,num_classes]
```

---

## ğŸ”ª Auto-Slicing System Workflow

```mermaid
flowchart TD
    START[ğŸ—‚ï¸ Large Mesh Input<br/>e.g., 2M faces]
    
    PARSE[Parse PLY File<br/>Extract vertices & faces]
    
    MODE{Slicing Mode?}
    
    GRID_GEN[Grid Mode<br/>â”â”â”â”â”â”â”â”<br/>Generate bounding boxes<br/>based on fixed divisions<br/>e.g., 3Ã—3Ã—1 = 9 boxes]
    
    ADAPTIVE_GEN[Adaptive Mode<br/>â”â”â”â”â”â”â”â”<br/>Calculate face density<br/>Generate variable boxes<br/>target_faces_per_box=5000]
    
    SLICE[Slice Mesh<br/>â”â”â”â”â”â”â”â”<br/>â€¢ Assign faces to boxes<br/>â€¢ Extract vertices<br/>â€¢ Remove unused vertices]
    
    SAVE[Write PLY Files<br/>â”â”â”â”â”â”â”â”<br/>mesh_slice_000.ply<br/>mesh_slice_001.ply<br/>...mesh_slice_NNN.ply]
    
    REUSE{Reuse existing<br/>slices?}
    
    LOAD[Load Existing Slices]
    
    INFER[Run Inference<br/>on each slice]
    
    AGGREGATE[Aggregate Results<br/>Combine predictions]
    
    OUTPUT[ğŸ’¾ Output<br/>â”â”â”â”â”â”â”â”<br/>Labeled mesh slices]
    
    START --> REUSE
    REUSE -->|Yes & exist| LOAD
    REUSE -->|No| PARSE
    PARSE --> MODE
    
    MODE -->|grid| GRID_GEN
    MODE -->|adaptive| ADAPTIVE_GEN
    
    GRID_GEN --> SLICE
    ADAPTIVE_GEN --> SLICE
    
    SLICE --> SAVE
    SAVE --> INFER
    LOAD --> INFER
    INFER --> AGGREGATE
    AGGREGATE --> OUTPUT
    
    style START fill:#FFE6E6,stroke:#CC0000,stroke-width:3px
    style MODE fill:#E6F3FF,stroke:#0066CC,stroke-width:2px
    style GRID_GEN fill:#FFF9E6,stroke:#CC9900,stroke-width:2px
    style ADAPTIVE_GEN fill:#FFF9E6,stroke:#CC9900,stroke-width:2px
    style SLICE fill:#FFE6F0,stroke:#CC0066,stroke-width:2px
    style INFER fill:#E6FFE6,stroke:#00CC66,stroke-width:2px
    style OUTPUT fill:#E6F0FF,stroke:#6600CC,stroke-width:3px
```

---

## ğŸ“¦ Dependencies (requirements.txt)

```bash
# Deep Learning Framework
torch>=2.0.0              # Core deep learning
torchvision>=0.15.0       # Vision utilities
tensorboard>=2.13.0       # Training visualization

# Mesh Processing
trimesh>=3.23.0           # 3D mesh manipulation
networkx>=2.8.0           # Graph operations (trimesh dependency)

# Numerical Computing
numpy>=1.24.0             # Array operations
scipy>=1.10.0             # Scientific computing (KDTree, MST)

# Machine Learning
scikit-learn>=1.3.0       # KMeans clustering

# Metrics and Evaluation
pytorch-ignite>=0.4.11    # IoU, ConfusionMatrix
torchmetrics>=1.0.0       # F1 Score, Accuracy

# Configuration
pyyaml>=6.0               # YAML config parsing

# Progress Bars
tqdm>=4.65.0              # Training progress

# Image Processing
Pillow>=10.0.0            # PIL for image handling

# Utilities
glob2>=0.7                # File pattern matching
```

### Installation

```bash
# Install all dependencies
pip install -r requirements.txt

# Or with conda
conda install --file requirements.txt
```

---

## ğŸ¯ Key Features & Capabilities

### âœ¨ Auto-Slicing System
| Feature | Description |
|---------|-------------|
| **Problem** | Large meshes (>1M faces) cause OOM errors |
| **Solution** | Automatic spatial decomposition into manageable chunks |
| **Grid Mode** | Fixed spatial divisions (e.g., 3Ã—3Ã—1 = 9 slices) |
| **Adaptive Mode** | Density-based slicing (target faces per box) |
| **Reusability** | Slices saved and reused across inference runs |
| **Vertex Cleanup** | Removes unused vertices for memory efficiency |

### ğŸ¨ Texture-Geometry Integration
| Component | Function |
|-----------|----------|
| **Dual Processing** | Separate branches for geometry and texture |
| **Pixel Embeddings** | Converts RGB pixel sequences to embeddings |
| **Cross-Modal Attention** | Geometry â†” Texture feature fusion |
| **CLS Token Pooling** | Summarizes pixel sequences per face |

### ğŸš€ Training Infrastructure
| Feature | Benefit |
|---------|---------|
| **EMA** | Exponential Moving Average for stability |
| **Mixed Precision** | Faster training with AMP (Automatic Mixed Precision) |
| **Checkpointing** | Resume training from saved states |
| **Multi-GPU** | Distributed training support |
| **Augmentation** | Rotation, scaling, jittering for robustness |

### ğŸ“Š Evaluation Metrics
- **IoU (Intersection over Union)** - Segmentation overlap
- **F1 Score** - Harmonic mean of precision/recall
- **Accuracy** - Overall classification correctness
- **Confusion Matrix** - Per-class performance

---

## ğŸš€ Quick Start Guide

### 1ï¸âƒ£ Installation

```bash
# Clone repository
cd CHemiAI

# Install dependencies
pip install -r requirements.txt
```

### 2ï¸âƒ£ Training

```bash
# Train geometry-only model
python train.py --config config_train_geometry.yaml

# Train integrated texture-geometry model
python train.py --config config_train_integrated.yaml
```

### 3ï¸âƒ£ Inference

```bash
# Standard inference
python Infer.py --config config_inference.yaml

# Inference with auto-slicing (for large meshes)
python Infer.py --config config_inference_auto_slice_example.yaml
```

---

## âš™ï¸ Configuration Examples

### Training Configuration
```yaml
model:
  type: 'integrated'        # or 'geometry_only'
  embedding_dim: 128
  num_heads: 4
  num_layers: 6
  dropout: 0.1
  num_classes: 10

training:
  batch_size: 8
  learning_rate: 0.001
  epochs: 100
  use_ema: true
  ema_decay: 0.9999
  mixed_precision: true

data:
  mesh_dir: '/path/to/meshes'
  texture_dir: '/path/to/textures'
  num_workers: 4
```

### Auto-Slice Inference Configuration
```yaml
inference:
  checkpoint: 'checkpoints/best_model.pth'
  output_dir: 'predictions/'
  batch_size: 4

auto_slice:
  enabled: true
  mode: 'grid'                # or 'adaptive'
  grid_divisions: [3, 3, 1]   # 9 slices
  reuse_existing_slices: true
  output_dir: 'sliced_meshes/'
  
  # Adaptive mode parameters (if mode='adaptive')
  # target_faces_per_box: 5000
  # min_boxes: 4
  # max_boxes: 50
```

---

## ğŸ“ˆ Performance Considerations

### Memory Optimization
- **Auto-Slicing**: Process large meshes in chunks
- **Vertex Removal**: Unused vertices removed from slices
- **Gradient Checkpointing**: Available for memory-constrained training
- **Mixed Precision**: Reduces memory by ~50%

### Speed Optimization
- **Batch Processing**: Process multiple meshes simultaneously
- **Reuse Slices**: Avoid re-slicing on subsequent runs
- **Multi-GPU**: Distributed training support
- **AMP**: Faster computation with automatic mixed precision

---

## ğŸ—‚ï¸ Data Format

### Input Mesh (.ply)
```
ply
format ascii 1.0
element vertex N
property float x
property float y
property float z
property float nx
property float ny
property float nz
element face M
property list uchar int vertex_indices
property int label
end_header
```

### Texture Data Format
```
{
  "face_id": 0,
  "pixels": [[R,G,B], [R,G,B], ...],
  "num_pixels": 256
}
```

---

## ğŸ” Project Statistics

| Metric | Count |
|--------|-------|
| **Python Files** | 16 |
| **Core Models** | 2 (nomeformer + integrated) |
| **Dataset Classes** | 2 (geometry + texture) |
| **Utility Modules** | 7 |
| **Configuration Files** | 2 YAML |
| **Documentation** | 2 Markdown files |

---

## ğŸ› ï¸ Technology Stack

<table>
<tr>
<td><b>Category</b></td>
<td><b>Technology</b></td>
<td><b>Purpose</b></td>
</tr>
<tr>
<td>Deep Learning</td>
<td>PyTorch 2.0+</td>
<td>Model architecture & training</td>
</tr>
<tr>
<td>3D Processing</td>
<td>Trimesh</td>
<td>Mesh manipulation & I/O</td>
</tr>
<tr>
<td>Numerical</td>
<td>NumPy + SciPy</td>
<td>Array operations & spatial algorithms</td>
</tr>
<tr>
<td>Clustering</td>
<td>scikit-learn</td>
<td>KMeans spatial partitioning</td>
</tr>
<tr>
<td>Metrics</td>
<td>Ignite + TorchMetrics</td>
<td>IoU, F1, Accuracy evaluation</td>
</tr>
<tr>
<td>Visualization</td>
<td>TensorBoard</td>
<td>Training monitoring</td>
</tr>
<tr>
<td>Config</td>
<td>YAML</td>
<td>Experiment configuration</td>
</tr>
</table>

---

## ğŸ“š Additional Resources

- **Auto-Slicing Documentation**: See `AUTO_SLICE_README.md`
- **Config Examples**: `config_inference_auto_slice_example.yaml`
- **Dependencies**: `requirements.txt`

---

## ğŸ“ Architecture Highlights

### ğŸ† Novel Components
1. **Hierarchical Attention**: Face â†’ Cluster â†’ Global
2. **Cross-Modal Fusion**: Geometry + Texture integration
3. **Auto-Slicing**: Handle arbitrarily large meshes
4. **Spatial Clustering**: KMeans + MST for spatial coherence

### ğŸ”¬ Research Features
- Exponential Moving Average (EMA) for training stability
- Mixed Precision Training for efficiency
- Relative Positional Encoding in attention
- Masked loss for variable-length sequences

---

<div align="center">

**ğŸ“Š Generated on: 2025-10-12**

*Repository diagram for CHemiAI - A mesh analysis framework with texture-geometry integration*

</div>
